{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sevi_multiModal import Sevi_multiModal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1708572522.829229       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 83), renderer: Apple M1 Pro\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "sevi_multiModal = Sevi_multiModal(trained_model='./save_models/trained_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error.\n",
      "error.\n",
      "error.\n",
      "error.\n",
      "error.\n"
     ]
    }
   ],
   "source": [
    "sevi_multiModal.specify_dataset(\n",
    "    data_type = 'csv',\n",
    "    table_name = 'nba_players',\n",
    "    data_url= './dataset/database/nba/nba_players.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sevi_multiModal.speech2vis('./speech/13.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n"
     ]
    }
   ],
   "source": [
    "!wget -q https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /Users/hyemisong/.pyenv/versions/3.9.1/envs/savi-3.9.1-1/lib/python3.9/site-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/hyemisong/.pyenv/versions/3.9.1/envs/savi-3.9.1-1/lib/python3.9/site-packages (from opencv-python) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1708570822.007410       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 83), renderer: Apple M1 Pro\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thumb Up Gesture Detected\n",
      "Thumb Up Gesture Detected\n",
      "Palm Open Gesture Detected\n",
      "Palm Open Gesture Detected\n",
      "Thumb Up Gesture Detected\n",
      "Thumb Up Gesture Detected\n",
      "Palm Open Gesture Detected\n",
      "Thumb Up Gesture Detected\n",
      "Palm Open Gesture Detected\n",
      "Thumb Up Gesture Detected\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Mediapipe 초기화\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# 제스처 인식을 위한 함수\n",
    "def recognize_gesture(hand_landmarks):\n",
    "    # 엄지와 검지의 끝 랜드마크\n",
    "    thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
    "    thumb_ip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_IP]\n",
    "\n",
    "    # 엄지척 판단\n",
    "    thumb_up = thumb_tip.y < thumb_ip.y\n",
    "\n",
    "    # 나머지 손가락 판단\n",
    "    fingers_up = [thumb_up]\n",
    "    for finger_tip_index, finger_mcp_index in [(8, 5), (12, 9), (16, 13), (20, 17)]:  # 각 손가락의 끝과 MCP 인덱스\n",
    "        finger_tip = hand_landmarks.landmark[finger_tip_index]\n",
    "        finger_mcp = hand_landmarks.landmark[finger_mcp_index]\n",
    "        if finger_tip.y < finger_mcp.y:\n",
    "            fingers_up.append(True)\n",
    "        else:\n",
    "            fingers_up.append(False)\n",
    "\n",
    "    # 엄지척 조건: 엄지만 위로\n",
    "    if fingers_up.count(True) == 1 and thumb_up:\n",
    "        return \"Thumb Up\"\n",
    "\n",
    "    # 손바닥 열림 조건: 모든 손가락이 위로\n",
    "    if all(fingers_up):\n",
    "        return \"Palm Open\"\n",
    "\n",
    "    return \"Unknown\"\n",
    "\n",
    "# 출력 빈도 조절을 위한 변수\n",
    "last_time_printed = time.time()\n",
    "print_interval = 2.0  # 초 단위\n",
    "\n",
    "# 웹캠 캡처 시작\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "        print(\"웹캠에서 이미지를 읽어오는데 실패했습니다.\")\n",
    "        break\n",
    "\n",
    "    # BGR 이미지를 RGB로 변환\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # Mediapipe에 전달하기 전에 이미지 쓰기를 방지\n",
    "    image.flags.writeable = False\n",
    "    # 손 감지\n",
    "    results = hands.process(image)\n",
    "\n",
    "    # 이미지를 RGB에서 BGR로 다시 변환하여 OpenCV에서 사용\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # 감지된 손 랜드마크 그리기\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "            gesture = recognize_gesture(hand_landmarks)\n",
    "            if gesture != \"Unknown\":\n",
    "                current_time = time.time()\n",
    "                if current_time - last_time_printed > print_interval:\n",
    "                    print(f\"{gesture} Gesture Detected\")\n",
    "                    last_time_printed = current_time\n",
    "\n",
    "    # 결과 이미지 표시\n",
    "    cv2.imshow('MediaPipe Hands', image)\n",
    "\n",
    "    # 'q' 키를 누르면 종료\n",
    "    if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# 자원 해제\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1708571379.675993       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 83), renderer: Apple M1 Pro\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Mediapipe 초기화\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# 제스처 인식을 위한 함수\n",
    "def recognize_gesture(hand_landmarks):\n",
    "    # 엄지와 검지의 끝 랜드마크\n",
    "    thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
    "    thumb_ip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_IP]\n",
    "\n",
    "    # 엄지척 판단\n",
    "    thumb_up = thumb_tip.y < thumb_ip.y\n",
    "\n",
    "    # 나머지 손가락 판단\n",
    "    fingers_up = [thumb_up]\n",
    "    for finger_tip_index, finger_mcp_index in [(8, 5), (12, 9), (16, 13), (20, 17)]:  # 각 손가락의 끝과 MCP 인덱스\n",
    "        finger_tip = hand_landmarks.landmark[finger_tip_index]\n",
    "        finger_mcp = hand_landmarks.landmark[finger_mcp_index]\n",
    "        if finger_tip.y < finger_mcp.y:\n",
    "            fingers_up.append(True)\n",
    "        else:\n",
    "            fingers_up.append(False)\n",
    "\n",
    "    # 엄지척 조건: 엄지만 위로\n",
    "    if fingers_up.count(True) == 1 and thumb_up:\n",
    "        return \"Thumb Up\"\n",
    "\n",
    "    # 손바닥 열림 조건: 모든 손가락이 위로\n",
    "    if all(fingers_up):\n",
    "        return \"Palm Open\"\n",
    "\n",
    "    return \"Unknown\"\n",
    "\n",
    "# 웹캠 캡처 시작\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "        print(\"웹캠에서 이미지를 읽어오는데 실패했습니다.\")\n",
    "        break\n",
    "\n",
    "    # BGR 이미지를 RGB로 변환\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # Mediapipe에 전달하기 전에 이미지 쓰기를 방지\n",
    "    image.flags.writeable = False\n",
    "    # 손 감지\n",
    "    results = hands.process(image)\n",
    "\n",
    "    # 이미지를 RGB에서 BGR로 다시 변환하여 OpenCV에서 사용\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # 감지된 손 랜드마크 그리기 및 제스처 인식\n",
    "    detected_gesture = \"Unknown\"  # 기본값은 'Unknown'\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "            # 제스처 인식 및 저장\n",
    "            detected_gesture = recognize_gesture(hand_landmarks)\n",
    "\n",
    "    # 감지된 제스처를 화면에 표시\n",
    "    cv2.putText(image, detected_gesture, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # 결과 이미지 표시\n",
    "    cv2.imshow('MediaPipe Hands', image)\n",
    "\n",
    "    # 'q' 키를 누르면 종료\n",
    "    if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# 자원 해제\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1708574456.397537       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 83), renderer: Apple M1 Pro\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error.\n",
      "error.\n",
      "error.\n",
      "error.\n",
      "error.\n"
     ]
    }
   ],
   "source": [
    "from Sevi_multiModal import Sevi_multiModal\n",
    "sevi_multiModal = Sevi_multiModal(trained_model='./save_models/trained_model.pt')\n",
    "sevi_multiModal.specify_dataset(\n",
    "    data_type = 'csv',\n",
    "    table_name = 'nba_players',\n",
    "    data_url= './dataset/database/nba/nba_players.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제스처 인식 시작\n",
    "sevi_multiModal.detect_gesture()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sevi thinks you said: show me the proportion of shot\n",
      "pred_querymark arc data nba_players encoding x shot y aggregate count shot transform group x\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'spec'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m sevi_multiModal\u001b[38;5;241m.\u001b[39mdetect_gesture()\n\u001b[0;32m----> 2\u001b[0m \u001b[43msevi_multiModal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspeech2vis\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#show me the proportion of shot\u001b[39;00m\n",
      "File \u001b[0;32m~/Dropbox/Pythonenv/savi-3.9.1-1/savitest/Sevi_multiModal.py:221\u001b[0m, in \u001b[0;36mSevi_multiModal.speech2vis\u001b[0;34m(self, AUDIO_FILE)\u001b[0m\n\u001b[1;32m    219\u001b[0m nl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeech2text(AUDIO_FILE)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m##return self.nl2vis(nl)[0]\u001b[39;00m\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnl2vis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnl\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dropbox/Pythonenv/savi-3.9.1-1/savitest/Sevi_multiModal.py:267\u001b[0m, in \u001b[0;36mSevi_multiModal.nl2vis\u001b[0;34m(self, nl_question, chart_template, show_progress, visualization_aware_translation)\u001b[0m\n\u001b[1;32m    265\u001b[0m     vega_lite_spec_json[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspec\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmark\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mline\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecognized_gesture \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThumb Up\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 267\u001b[0m     \u001b[43mvega_lite_spec_json\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspec\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmark\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbar\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# 동적으로 업데이트된 스펙을 사용하여 차트 렌더링\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m display({\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/vnd.vegalite.v5+json\u001b[39m\u001b[38;5;124m\"\u001b[39m: vega_lite_spec_json\n\u001b[1;32m    272\u001b[0m }, raw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'spec'"
     ]
    }
   ],
   "source": [
    "sevi_multiModal.detect_gesture()\n",
    "# sevi_multiModal.speech2vis()\n",
    "\n",
    "#show me the proportion of shot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "savi-3.9.1-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
